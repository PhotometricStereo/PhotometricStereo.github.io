<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>DiLiGentPi</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="icon" type="image/png" href="./src/iconPS.png">
    <link rel="stylesheet" href="./src/bootstrap.min.css">
    <link rel="stylesheet" href="./src/font-awesome.min.css">
    <link rel="stylesheet" href="./src/codemirror.min.css">
    <link rel="stylesheet" href="./src/app.css">
    <link rel="stylesheet" href="./src/bootstrap.min(1).css">

    <!-- <script type="text/javascript" async="" src="./src/analytics.js"></script>
    <script type="text/javascript" async="" src="./src/analytics(1).js"></script>
    <script async="" src="./src/js"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-110862391-3');
    </script> -->

    <!-- <script src="./src/jquery.min.js"></script>
    <script src="./src/bootstrap.min.js"></script>
    <script src="./src/codemirror.min.js"></script>
    <script src="./src/clipboard.min.js"></script>

    <script src="./src/app.js"></script> -->
</head>

<body data-gr-c-s-loaded="true">
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                DiLiGent-Pi: Photometric Stereo for Planar Surfaces with Rich Details –</br>
                Benchmark Dataset and Beyond
            <br /><br />
            <small>
                ICCV 2023 (<b>Poster Presentation</b>)
            </small>
            <br /><br />
            </h1>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://photometricstereo.github.io/diligent102.html">
                          Feishi Wang
                        </a><sup>1,2,3,†</sup>
                    </li>                  
                    <li>
                        <a href="https://photometricstereo.github.io/diligent102.html">
                          Jieji Ren
                        </a><sup>4,†</sup>
                    </li>
                    <li>
                        <a href="https://gh-home.github.io/">
                          Heng Guo
                        </a><sup>5,6,†</sup>
                    </li>
                    <li>
                        <a href="https://me.sjtu.edu.cn/teacher_directory1/renmingjun.html">
                          Mingjun Ren
                        </a><sup>4,‡</sup>
                    </li>
                    <li>
                        <a href="https://ci.idm.pku.edu.cn/People.htm">
                          Boxin Shi
                        </a><sup>1,2,3,‡</sup>
                    </li> 
                </ul>
            </div>
        </div>

        
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <sup>1</sup>National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University
                    </li>
                    <li>
                        <sup>2</sup>National Engineering Research Center of Visual Technology, School of Computer Science, Peking University
                    </li>
                    <li>
                        <sup>3</sup>AI Innovation Center, School of Computer Science, Peking University
                    </li>                    
                    <li>
                        <sup>4</sup>School of Mechanical Engineering, Shanghai Jiao Tong University
                    </li>                    
                    <li>
                        <sup>5</sup>School of Artificial Intelligence, Beijing University of Posts and Telecommunications
                    </li>
                    <li>
                        <sup>6</sup>Osaka University
                    </li>
                </ul>
                <br /><br />
            </div>
        </div>

        <div class="row">
                <div class="col-md-8 col-md-offset-2 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://photometricstereo.github.io/imgs/diligentpi/paper.pdf">
                            <img src="./imgs/diligentpi/paperface_v0.1.png" height="120px"><br>
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://disk.pku.edu.cn/link/AAD1BF9F1CC9F946EF8AE993537D8859C2">
                            <img src="./imgs/diligentpi/suppface_v0.1.png" height="120px"><br>
                                <h4><strong>Supplementary</strong></h4>
                            </a>
                        </li>
                        <li>
                            <!--a href="https://lab.ybh1998.space:8443/pspibenchmarkwebsite/"-->
                            <a href="https://www.kaggle.com/datasets/zaiyanyang/diligent-pi">
                            <img src="./imgs/diligentpi/evalogo.png" height="120px"><br>
                                <h4><strong>Evaluation</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://disk.pku.edu.cn/link/AA5B7383D75C514F6C8E3727CD3D8384A2">
                            <img src="./imgs/diligentpi/datalogo.png" height="120px"><br>
                                <h4><strong>Dataset</strong></h4>
                            </a>
                        </li>
                    </ul>
                    <br /><br /><br />
                </div>
        </div>




        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Overview
                </h3>
                <img src="./imgs/diligentpi/piw.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    Recovering the surface details is one of the potentional advantages of Photometric Stereo over other 3D reconstruction techniques. However, there is no existing real-world datasets focus on near-planar surfaces width rich detials, which widely exist in cultural relics and manufacturing workpieces. To fill this missing part of photometrc stereo, we present a new real-world dataset DiLiGenT-Π containing 30 nearplanar objects with rich surface details. This dataset enables us to evaluate recent photometric stereo methods specifically for their ability to estimate shape details under diverse materials and to identify open problems, such as near-planar surface normal estimation from uncalibrated photometric stereo and surface detail recovery for translucent materials.
                </p>
            </div>
        </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Highlights
                </h3>
                <p class="text-justify">
                    <ul>
                        <li>
                            First public near-planar rich-details PS dataset with 30 sets of images with `ground truth` normal; 
                        </li>
                        <li>
                            Complex geometric surface and diverse materials are designed;
                        </li>
                        <li>
                            A lightweight capture system are developed for small objects observation with omnidirectional illumination;
                        </li>
                        <li>
                            Evaluating details recovery performance of PS methods and identifing new open problems.
                        </li>
                    </ul>
                </p>
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Dateset Capture Setup and `GT` Generate Process
                </h3>
                <img src="./imgs/diligentpi/gtprocesss.png" class="img-responsive" alt="captureimg"><br>           
                <p class="text-justify">
                    Lightweight illumination and imaging setup for capture the DiLiGenT-Pi dataset. The system is consist of two-axes rotation stage and LED for omnidirectional illumination, mirrow balls for light direction calibration, camera for image capturing, and black felt board coverd cage acts as darkroom. We take <a href="https://www.alicona.com/en/products/infinitefocus">Alicona</a> to measure the accurate surface point cloud of objects, and take the <a href="https://www.blender.org/">Blender</a> to register point cloud with observed image, finally rendering the surface normal map as `ground truth`.
                </p>
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Benchmark Results
                </h3>
                <img src="./imgs/diligentpi/benchmark_new.png" class="img-responsive" alt="benchmark"><br>
                <p class="text-justify">
                    A 15*30 algo-obj error distribution matrix is visualized to depict the recovery perfoemance of each PS method. 
                </p>
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Reconvery Performance on Selected Methods
                </h3>
                <img src="./imgs/diligentpi/compare.png" class="img-responsive" alt="compare"><br>
                <p class="text-justify">
                    Visualization of surface detail recovery from different photometric stereo methods. Per-pixel based photometric stereo method is more effective on detail recovery compared to all-pixel based method.
                </p>
                <img src="./imgs/diligentpi/compups.png" class="img-responsive" alt="compareups"><br>
                <p class="text-justify">
                    Surface normal estimates of photometric stereo methods with (top row) or without (bottom row) finetuning on PS_RELIEF. Compared to calibrated method, learning-based uncalibrated photometric stereo are heavily influenced by the shape prior learned from the training dataset.
                </p>               
            </div>
        </div>

        
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citataion
                </h3>
                <p style="border-style: groove;border-width: 1px;border-color: lightgrey;color:grey;">
                    &nbsp;@InProceedings{Wang_Ren_Guo_2023_ICCV,</br>
                        &nbsp;author = {Wang, Feishi and Ren, Jieji and Guo, Heng and Ren, Mingjun and Shi, Boxin},</br>
                        &nbsp;title = {DiLiGenT-Pi: A Photometric Stereo Benchmark Dataset with Controlled Shape and Material Variation},</br>
                        &nbsp;booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},</br>
                        &nbsp;month = {October},</br> 
                        &nbsp;year = {2023},</br> 
                        &nbsp;pages = {9477-9487}</br>
                        }</br> 
                </p>
    
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Contact
                </h3>
                <p class='text-justify'>Any questions and further discussion, please send e-mail to:<br> <a>wangfeishi_AT_pku_DOT_edu_DOT_cn</a>.
                </p>
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgments
                </h3>
                We acknowledge support from National Natural Science Foundation of China and computation resource from openbayes.com. The website template was borrowed from <a href="https://vilab-ucsd.github.io/ucsd-openrooms/">OpenRooms</a>.
                <p></p>
            </div>
        </div>
    
    </div>


</body></html>
